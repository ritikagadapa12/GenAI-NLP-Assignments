{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUtw4eIte55R",
        "outputId": "b67c547c-88b0-46f8-e2af-8cbe0bf4b53b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.tokenize import WhitespaceTokenizer, TreebankWordTokenizer, PunktSentenceTokenizer, TweetTokenizer, MWETokenizer\n",
        "\n",
        "WT = WhitespaceTokenizer()\n",
        "DT = TreebankWordTokenizer()\n",
        "PT = PunktSentenceTokenizer()\n",
        "TT = TweetTokenizer()\n",
        "MW = MWETokenizer()\n",
        "\n",
        "sent = \"This is a random sentence generated using human intelligence. (trial)\"\n",
        "\n",
        "print(WT.tokenize(sent))\n",
        "print(DT.tokenize(sent))\n",
        "print(PT.tokenize(sent))\n",
        "print(TT.tokenize(sent))\n",
        "print(MW.tokenize(sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdwWAQRCe-Gp",
        "outputId": "bd33df77-31a1-400e-bec9-e7e9393fa64c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'random', 'sentence', 'generated', 'using', 'human', 'intelligence.', '(trial)']\n",
            "['This', 'is', 'a', 'random', 'sentence', 'generated', 'using', 'human', 'intelligence.', '(', 'trial', ')']\n",
            "['This is a random sentence generated using human intelligence.', '(trial)']\n",
            "['This', 'is', 'a', 'random', 'sentence', 'generated', 'using', 'human', 'intelligence', '.', '(', 'trial', ')']\n",
            "['T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', ' ', 'r', 'a', 'n', 'd', 'o', 'm', ' ', 's', 'e', 'n', 't', 'e', 'n', 'c', 'e', ' ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'e', 'd', ' ', 'u', 's', 'i', 'n', 'g', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'i', 'n', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'n', 'c', 'e', '.', ' ', '(', 't', 'r', 'i', 'a', 'l', ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "snowball = SnowballStemmer(language='english')\n",
        "\n",
        "words = [\"running\", \"runner\", \"runs\", \"swimming\", \"swimmer\", \"swims\"]\n",
        "stemmed_words = [porter.stem(word) for word in words]\n",
        "\n",
        "print(\"Porter Stemmer Output:\")\n",
        "for original, stemmed in zip(words, stemmed_words):\n",
        "    print(f\"{original} -> {stemmed}\")\n",
        "\n",
        "print(\"\\nSnowball Stemmer Output:\")\n",
        "for original, stemmed in zip(words, stemmed_words):\n",
        "    print(f\"{original} -> {stemmed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEHr5EhcfAq1",
        "outputId": "b9c7ecd4-2647-46f2-c47d-e211a1a45bdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer Output:\n",
            "running -> run\n",
            "runner -> runner\n",
            "runs -> run\n",
            "swimming -> swim\n",
            "swimmer -> swimmer\n",
            "swims -> swim\n",
            "\n",
            "Snowball Stemmer Output:\n",
            "running -> run\n",
            "runner -> runner\n",
            "runs -> run\n",
            "swimming -> swim\n",
            "swimmer -> swimmer\n",
            "swims -> swim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        "\n",
        "# a denotes adjective in \"pos\"\n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dS94XArfNMv",
        "outputId": "77065d3a-6cb3-43e2-d5b8-34b74e410919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rocks : rock\n",
            "corpora : corpus\n",
            "better : good\n"
          ]
        }
      ]
    }
  ]
}